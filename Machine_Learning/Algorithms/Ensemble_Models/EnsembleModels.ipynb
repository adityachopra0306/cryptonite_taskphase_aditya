{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a868fb60-d05d-4922-bf07-c5f988b05468",
   "metadata": {},
   "source": [
    "# Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3f9256-d0ab-48e2-97ad-b07fa8097659",
   "metadata": {},
   "source": [
    "Ensemble models are a class of machine learning techniques that combine multiple individual models (often called \"weak learners\") to create a stronger model. This technique can improve the performance of predictive models by reducing bias and variance. There are 2 types of Ensemble learning algorithms:\n",
    "## 1. Bagging\n",
    "These are variance reduction models commonly used for overfitting models. Bagging, or **Bootstrap Aggregation** takes out random samples from the data and builds each model taking a bootstrap sample of the data as training example. Prediction is done by taking the mode of the labels from each model in case of classification or average of the target values in case of regression models.\n",
    "\n",
    "## 2. Boosting\n",
    "This is a Bias reduction model that combines weak learners to get a reliable learning algorithm. It works by sequentially training models, where each subsequent model corrects the errors made by the previous one. In each iteration, the algorithm assigns a weight to the data points that were misclassified by the previous model, ensuring that the new model now focuses on those instances more, hece fitting the data better.\n",
    "\n",
    "Mathematically, boosting creates a final model $H(x)$ as a weighted sum of the individual weak models $h_t(x)$:\n",
    "\n",
    "$$\n",
    "H(x) = \\sum_{t=1}^{T} \\alpha_t h_t(x)\n",
    "$$\n",
    "where $T$ is the number of models and $\\alpha_t$ is the weight of each model $h_t(x)$.\n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "AdaBoost, or **Adaptive Boosting** is one of the most popular boosting algorithms. It changes weights of the datapoints after training each tree, giving more weight to instances that were misclassified by previous models. In AdaBoost, each model is assigned a weight based on its accuracy, and the final prediction is made by taking a weighted majority vote (for classification) or a weighted sum (for regression).\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "#### 1. Initialize weights\n",
    "Start with equal weights for all training samples: \n",
    "   \n",
    "   $$ w_1 = w_2 = \\dots = w_m = \\frac{1}{m} $$\n",
    "\n",
    "   where $m$ is the total number of training examples.\n",
    "\n",
    "#### 2. Train weak model\n",
    "Train a weak model $h_t(x)$ using the weighted datapoints. (For decision trees, they are made weak learners by forcing the model to make only one split.)\n",
    "\n",
    "#### 3. Calculate error\n",
    "Compute the error rate $err_t$ of the model $h_t(x)$ on the weighted dataset:\n",
    "\n",
    "   $$ err_t = \\frac{\\sum_{i=1}^{m} w_i \\cdot I(y_i \\neq h_t(x_i))}{\\sum_{i=1}^{m} w_i} $$\n",
    "\n",
    "   Where $I$ is the indicator function.\n",
    "   \n",
    "#### 4. Compute model weight\n",
    "Calculate the weight $\\alpha_t$ of the model $h_t(x)$:\n",
    "\n",
    "   $$ \\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1-err_t}{err_t}\\right) $$\n",
    "\n",
    "#### 5. Update sample weights\n",
    "\n",
    "   $$ w_{i} := w_i \\cdot \\exp\\left(\\alpha_t \\cdot I(y_i \\neq h_t(x_i))\\right) $$\n",
    "\n",
    "#### 6. Prediction\n",
    "\n",
    "   $$ H(x) = \\text{sgn}\\left(\\sum_{t=1}^{T} \\alpha_t h_t(x)\\right) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa38bf8-e10f-4b60-8987-f26c394f60a3",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "The following is an implementation of the Adaboost Algorithm on the Breast Cancer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74dda98-8495-4036-ba61-c6962c29fc48",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7636bac3-17eb-45c0-b869-ce196a937469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import mode\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c16fd0-4c93-489e-880f-60be5d7cd230",
   "metadata": {},
   "source": [
    "### Creating Weighted Decision Tree\n",
    "\n",
    "For a weighted binary decision tree, the gini impurity is:\n",
    "$$\n",
    "G(S) = 1 - \\sum_{c \\in \\{S_l,S_r\\}} \\left( \\frac{\\sum_{s \\in S_c} w_s}{\\sum_{s \\in S} w_s} \\right)^2,\n",
    "$$\n",
    "The information Gain:\n",
    "$$\n",
    "IG(S) = G(S) - \\left( \\frac{\\sum_{s \\in S_l} w_s}{\\sum_{s \\in S} w_s} G(S_l) + \\frac{\\sum_{s \\in S_r} w_s}{\\sum_{s \\in S} w_s} G(S_r) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e29acf5-f159-4fc2-9399-f2fe6b20a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, label=None):\n",
    "        self.j = feature_index\n",
    "        self.t = threshold\n",
    "        self.l = left\n",
    "        self.r = right\n",
    "        self.label = label\n",
    "\n",
    "class Weighted_DT:\n",
    "    def __init__(self, max_depth = 5):\n",
    "        self.max_d = max_depth\n",
    "        \n",
    "    def gini(self, y, mask = True):\n",
    "        classes = np.unique(y)\n",
    "        w = []\n",
    "        for c in classes:\n",
    "            w.append(np.sum(self.w[mask][y[mask]==c])/np.sum(self.w[mask]))\n",
    "        w = np.array(w)\n",
    "        return 1-np.sum(w**2)\n",
    "        \n",
    "    def info_gain(self, y, lm, rm):\n",
    "        return self.gini(y)-((np.sum(self.w[lm]))/np.sum(self.w) * self.gini(y,lm) + (np.sum(self.w[rm]))/np.sum(self.w) * self.gini(y,rm))\n",
    "    \n",
    "    def split(self,X,j,t):\n",
    "        l_mask = X[:,j] <= t\n",
    "        return l_mask, ~l_mask\n",
    "    \n",
    "    def best_split(self,X,y):\n",
    "        best_j, best_t = None, None\n",
    "        max_ig = 0\n",
    "        \n",
    "        for j in range(X.shape[1]):\n",
    "            possible_t = X[:,j]\n",
    "            \n",
    "            for t in possible_t:\n",
    "                lm,rm = self.split(X,j,t)\n",
    "                l, r = y[lm],y[rm]\n",
    "                if len(l)==0 or len(r)==0:\n",
    "                    continue\n",
    "                \n",
    "                ig = self.info_gain(y, lm, rm)\n",
    "                \n",
    "                if ig > max_ig:\n",
    "                    max_ig = ig\n",
    "                    best_j = j\n",
    "                    best_t = t\n",
    "        return best_j, best_t\n",
    "\n",
    "    def make_tree(self,X,y, curr_d=0):\n",
    "        if len(np.unique(y)) == 1 or curr_d >= self.max_d or len(y) < 5:\n",
    "            majority_class = np.bincount(y).argmax()\n",
    "            return Node(label=majority_class)\n",
    "            \n",
    "        j,t = self.best_split(X,y)\n",
    "        \n",
    "        if j==None:\n",
    "            majority_class = np.bincount(y).argmax()\n",
    "            return Node(label=majority_class)\n",
    "        \n",
    "        lm,rm = self.split(X,j,t)\n",
    "        l = self.make_tree(X[lm],y[lm],curr_d+1)\n",
    "        r = self.make_tree(X[rm],y[rm],curr_d+1)\n",
    "        return Node(j,t,l,r)\n",
    "\n",
    "    def train(self, X, y, w=None):\n",
    "        if w is None:\n",
    "            self.w = np.full((X.shape[0],),1/X.shape[0])\n",
    "        else:\n",
    "            self.w = w\n",
    "        self.root = self.make_tree(X,y)\n",
    "    \n",
    "    def predict_single(self, x, node):\n",
    "        if node.label is not None:\n",
    "            return node.label\n",
    "        if x[node.j] <= node.t:\n",
    "            return self.predict_single(x,node.l)\n",
    "        else:\n",
    "            return self.predict_single(x,node.r)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return np.array([self.predict_single(x, self.root) for x in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b6b4b-31d2-4fa8-9b18-4a155a8aafd2",
   "metadata": {},
   "source": [
    "### Implementing AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2bdaf89-a88d-4147-a3ef-ca6e2568fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaboost:\n",
    "    def __init__(self, tree_ct=10):\n",
    "        self.T = tree_ct\n",
    "        self.trees = []\n",
    "        self.alphas = []\n",
    "\n",
    "\n",
    "    def train(self,X,y):\n",
    "        self.w = np.full((X.shape[0],),1/X.shape[0])\n",
    "        for i in range(self.T):\n",
    "            tree = Weighted_DT(max_depth=1)\n",
    "            tree.train(X,y,self.w)\n",
    "\n",
    "            pred = tree.predict(X)\n",
    "            err = np.sum(self.w * (y!=pred))/np.sum(self.w)\n",
    "            alpha = np.log((1 - err)/err)\n",
    "            \n",
    "            self.alphas.append(alpha)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            self.w = self.w * np.exp(alpha * (y!=pred))\n",
    "\n",
    "    def predict(self,X):\n",
    "        predictions = np.empty(X.shape[0])\n",
    "        for tree, alpha in zip(self.trees, self.alphas):\n",
    "            predictions += tree.predict(X) * alpha\n",
    "        return predictions > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671723fd-0239-4c11-98a9-62f2a8125ae8",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e908902-54b0-47e4-a43a-6029e99675ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a8e5a8-75a5-451e-99b8-a27e3468b897",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = df.to_numpy()\n",
    "\n",
    "np.random.shuffle(dat)\n",
    "X = dat[:450,:-1]\n",
    "y = dat[:450,-1].astype(int)\n",
    "y_test = dat[450:,-1].astype(int)\n",
    "X_test = dat[450:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1a8601-9530-4438-b09d-49873882ce46",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26e6741c-0743-4881-8b6d-5f887e1b5760",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = Adaboost(tree_ct=40)\n",
    "ada.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1350930-43be-47f9-b5a9-c4f205decd2d",
   "metadata": {},
   "source": [
    "### Prediction and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ac89e1-2bb1-439a-bbb1-0c14a167e042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[21 27]\n",
      " [ 0 71]]\n",
      "\n",
      "Accuracy: 0.773109243697479\n",
      "Precision: 0.7244897959183674\n",
      "Recall: 1.0\n",
      "F1 Score: 0.8402366863905325\n"
     ]
    }
   ],
   "source": [
    "predictions = ada.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(y_test, predictions), end = \"\\n\\n\")\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
